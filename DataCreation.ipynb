{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfSpux0OcT3v",
        "outputId": "3cf6487c-5f2a-4cac-c3f4-14fbf75df184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.12/dist-packages (6.0.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from wikipedia-api) (2.32.4)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2025.10.5)\n"
          ]
        }
      ],
      "source": [
        "pip install wikipedia-api feedparser pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data collection**"
      ],
      "metadata": {
        "id": "VanqpohegXbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import re\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "FRg0pE5lcUYW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wikipedia Collection**\n",
        "\n",
        "Define categories and topics (wiki_categories)\n",
        "\n",
        "Four main categories: Machine Learning, Climate Change, Sports, Politics\n",
        "\n",
        "Each has a list of seed topics (e.g., Artificial neural network, Football).\n",
        "\n",
        "These mirror of project’s requirement to collect a topic-diverse dataset.\n",
        "\n",
        " **function(get_links)**\n",
        "\n",
        "it Starts from a Wikipedia page (e.g., “Climate change”).\n",
        "\n",
        "Collects its title, summary, and full text.\n",
        "\n",
        "Also collects related pages via internal links - this expands coverage naturally.\n",
        "\n",
        "Stops once the requested number of articles (max_articles) is reached.\n",
        "\n",
        "**Loop through categories & topics**\n",
        "\n",
        "For each topic, get_links is called.\n",
        "\n",
        "Articles are saved into a list of dictionaries with metadata:\n",
        "\n",
        "title, source = Wikipedia, category, content.\n",
        "\n",
        "A short pause (time.sleep(0.1)) is added - polite scraping etiquette.\n",
        "\n",
        "Thousands of Wikipedia pages organized by category."
      ],
      "metadata": {
        "id": "MBoUcYEexhJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- Wikipedia Setup -----------\n",
        "wiki = wikipediaapi.Wikipedia(\n",
        "    language='en',\n",
        "    user_agent='RanjithaNagaraj-MScDSProject/1.0 (ranjithanagaraj911@gmail.com)'\n",
        ")\n",
        "\n",
        "\n",
        "wiki_categories = {\n",
        "    \"Machine learning\": [\n",
        "        \"Artificial neural network\", \"Supervised learning\", \"Unsupervised learning\"\n",
        "    ],\n",
        "    \"Climate change\": [\n",
        "        \"Climate change\", \"Carbon dioxide\", \"Climate change mitigation\", \"Greenhouse gas\"\n",
        "    ],\n",
        "    \"Sports\": [\n",
        "        \"Sports\", \"Football\", \"Olympic Games\", \"Basketball\", \"Cricket\"\n",
        "    ],\n",
        "    \"Politics\": [\n",
        "        \"Political science\", \"Elections\", \"Political party\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "wikipedia_docs = []\n",
        "max_per_topic = 3000  # Increase/decrease to tune total dataset size\n",
        "\n",
        "def get_links(page_title, max_articles):\n",
        "    page = wiki.page(page_title)\n",
        "    articles = []\n",
        "    # Collect the main page\n",
        "    if page.exists() and len(page.summary) > 100:\n",
        "        articles.append((page.title, page.summary + \"\\n\" + page.text))\n",
        "    # Add links from this page (sub-articles)\n",
        "    links = list(page.links.keys())\n",
        "    for link in links[:max_articles - 1]:\n",
        "        linked_page = wiki.page(link)\n",
        "        if linked_page.exists() and len(linked_page.summary) > 100:\n",
        "            articles.append((linked_page.title, linked_page.summary + \"\\n\" + linked_page.text))\n",
        "        if len(articles) >= max_articles:\n",
        "            break\n",
        "    return articles\n",
        "\n",
        "# Scrape Wikipedia\n",
        "for category, topics in wiki_categories.items():\n",
        "    for topic in topics:\n",
        "        try:\n",
        "            print(f\"Extracting Wikipedia articles for: {topic} ({category})\")\n",
        "            articles = get_links(topic, max_per_topic // len(topics))\n",
        "            for title, content in articles:\n",
        "                wikipedia_docs.append({\n",
        "                    \"title\": title,\n",
        "                    \"source\": \"Wikipedia\",\n",
        "                    \"category\": category,\n",
        "                    \"content\": content\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Failed for {topic}: {e}\")\n",
        "        time.sleep(0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcy-HMbncUbI",
        "outputId": "ff8a05bc-796f-47ff-bf25-64f7b5261b64"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Wikipedia articles for: Artificial neural network (Machine learning)\n",
            "Extracting Wikipedia articles for: Supervised learning (Machine learning)\n",
            "Extracting Wikipedia articles for: Unsupervised learning (Machine learning)\n",
            "Extracting Wikipedia articles for: Climate change (Climate change)\n",
            "Extracting Wikipedia articles for: Carbon dioxide (Climate change)\n",
            "Extracting Wikipedia articles for: Climate change mitigation (Climate change)\n",
            "Extracting Wikipedia articles for: Greenhouse gas (Climate change)\n",
            "Extracting Wikipedia articles for: Sports (Sports)\n",
            "Extracting Wikipedia articles for: Football (Sports)\n",
            "Extracting Wikipedia articles for: Olympic Games (Sports)\n",
            "Extracting Wikipedia articles for: Basketball (Sports)\n",
            "Extracting Wikipedia articles for: Cricket (Sports)\n",
            "Extracting Wikipedia articles for: Political science (Politics)\n",
            "Extracting Wikipedia articles for: Elections (Politics)\n",
            "Extracting Wikipedia articles for: Political party (Politics)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "P3TboBDtSIAp",
        "outputId": "50442e5a-3b02-40ce-b5ab-7fb7619c3a56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RSS News Collection**\n",
        "\n",
        "**Define RSS feed sources (rss_feeds)**\n",
        "\n",
        "BBC feeds: Technology, Health, Science, World\n",
        "\n",
        "Guardian feeds: UK News, World\n",
        "\n",
        "These adds real-world, up-to-date news articles to the dataset.\n",
        "\n",
        "**Parse feeds**\n",
        "\n",
        "Use feedparser to download RSS feeds.\n",
        "\n",
        "For each entry: collect title + summary.\n",
        "\n",
        "Store in a dictionary:\n",
        "\n",
        "title, source = RSS, category (feed name), content.\n",
        "\n",
        "Hundreds of fresh news articles with summaries."
      ],
      "metadata": {
        "id": "x5D4drIPx6qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- RSS Setup -----------\n",
        "rss_feeds = {\n",
        "    \"BBC_Technology\": \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"BBC_Health\": \"http://feeds.bbci.co.uk/news/health/rss.xml\",\n",
        "    \"BBC_Science\": \"http://feeds.bbci.co.uk/news/science_and_environment/rss.xml\",\n",
        "    \"BBC_World\": \"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
        "    \"Guardian_UK_News\": \"https://www.theguardian.com/uk-news/rss\",\n",
        "    \"Guardian_World\": \"https://www.theguardian.com/world/rss\"\n",
        "}\n",
        "\n",
        "rss_docs = []\n",
        "\n",
        "for category, url in rss_feeds.items():\n",
        "    print(f\"Parsing RSS feed: {category}\")\n",
        "    feed = feedparser.parse(url)\n",
        "    for entry in feed.entries:\n",
        "        content = entry.title + \". \" + (entry.summary if 'summary' in entry else \"\")\n",
        "        rss_docs.append({\n",
        "            \"title\": entry.title,\n",
        "            \"source\": \"RSS\",\n",
        "            \"category\": category,\n",
        "            \"content\": content\n",
        "        })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYlwgu4OcUd_",
        "outputId": "8ea95404-931d-4d9d-cfb6-0814c03e3b46"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing RSS feed: BBC_Technology\n",
            "Parsing RSS feed: BBC_Health\n",
            "Parsing RSS feed: BBC_Science\n",
            "Parsing RSS feed: BBC_World\n",
            "Parsing RSS feed: Guardian_UK_News\n",
            "Parsing RSS feed: Guardian_World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine Wikipedia + RSS data into one pandas DataFrame.\n",
        "\n",
        "Remove duplicates (by title) to avoid redundancy.\n",
        "\n",
        "Reset index - keep dataset tidy.And\n",
        "\n",
        "Save dataset to combined_corpus.csv."
      ],
      "metadata": {
        "id": "bG_nzzhTyvFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- Combine and Save -----------\n",
        "# Combine both datasets, remove duplicates by title\n",
        "all_docs = pd.DataFrame(wikipedia_docs + rss_docs)\n",
        "#all_docs.drop_duplicates(subset=[\"title\"], inplace=True)\n",
        "all_docs.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(f\"Total unique documents: {len(all_docs)}\")\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "all_docs.to_csv(\"/content/drive/MyDrive/Ranjitha-DS-Project/combined_dataset_New.csv\", index=False)\n",
        "print(\"Saved as combined_dataset_New.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smpD8VaycUhH",
        "outputId": "99ce1160-5b3d-46e9-99bd-a0b81f02e114"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique documents: 8577\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Saved as combined_dataset_New.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R9HfnBYdgYG6"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}