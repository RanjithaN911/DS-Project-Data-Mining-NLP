{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfSpux0OcT3v",
        "outputId": "b1ed06df-7b00-4f32-fd5c-abec1c804611"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia-api\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from wikipedia-api) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->wikipedia-api) (2025.8.3)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: wikipedia-api, sgmllib3k\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15383 sha256=660378c3cede022ed41854fc48a280a7217aa5bfed327dd00653af719ae10dec\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=dd40f40f4904634cc82f2d684a2a813b846e523548fd15cf60cf50888cc66aaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/3b/25/2a/105d6a15df6914f4d15047691c6c28f9052cc1173e40285d03\n",
            "Successfully built wikipedia-api sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, wikipedia-api\n",
            "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0 wikipedia-api-0.8.1\n"
          ]
        }
      ],
      "source": [
        "pip install wikipedia-api feedparser pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data collection**"
      ],
      "metadata": {
        "id": "VanqpohegXbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "import feedparser\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "FRg0pE5lcUYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wikipedia Collection**\n",
        "\n",
        "Define categories and topics (wiki_categories)\n",
        "\n",
        "Four main categories: Machine Learning, Climate Change, Sports, Politics\n",
        "\n",
        "Each has a list of seed topics (e.g., Artificial neural network, Football).\n",
        "\n",
        "These mirror of project’s requirement to collect a topic-diverse dataset.\n",
        "\n",
        " **function(get_links)**\n",
        "\n",
        "it Starts from a Wikipedia page (e.g., “Climate change”).\n",
        "\n",
        "Collects its title, summary, and full text.\n",
        "\n",
        "Also collects related pages via internal links - this expands coverage naturally.\n",
        "\n",
        "Stops once the requested number of articles (max_articles) is reached.\n",
        "\n",
        "**Loop through categories & topics**\n",
        "\n",
        "For each topic, get_links is called.\n",
        "\n",
        "Articles are saved into a list of dictionaries with metadata:\n",
        "\n",
        "title, source = Wikipedia, category, content.\n",
        "\n",
        "A short pause (time.sleep(0.1)) is added - polite scraping etiquette.\n",
        "\n",
        "Thousands of Wikipedia pages organized by category."
      ],
      "metadata": {
        "id": "MBoUcYEexhJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- Wikipedia Setup -----------\n",
        "wiki = wikipediaapi.Wikipedia(\n",
        "    language='en',\n",
        "    user_agent='RanjithaNagaraj-MScDSProject/1.0 (ranjithanagaraj911@gmail.com)'\n",
        ")\n",
        "\n",
        "\n",
        "wiki_categories = {\n",
        "    \"Machine learning\": [\n",
        "        \"Artificial neural network\", \"Supervised learning\", \"Unsupervised learning\"\n",
        "    ],\n",
        "    \"Climate change\": [\n",
        "        \"Climate change\", \"Carbon dioxide\", \"Climate change mitigation\", \"Greenhouse gas\"\n",
        "    ],\n",
        "    \"Sports\": [\n",
        "        \"Sports\", \"Football\", \"Olympic Games\", \"Basketball\", \"Cricket\"\n",
        "    ],\n",
        "    \"Politics\": [\n",
        "        \"Political science\", \"Elections\", \"Political party\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "wikipedia_docs = []\n",
        "max_per_topic = 3000  # Increase/decrease to tune total dataset size\n",
        "\n",
        "def get_links(page_title, max_articles):\n",
        "    page = wiki.page(page_title)\n",
        "    articles = []\n",
        "    # Collect the main page\n",
        "    if page.exists() and len(page.summary) > 100:\n",
        "        articles.append((page.title, page.summary + \"\\n\" + page.text))\n",
        "    # Add links from this page (sub-articles)\n",
        "    links = list(page.links.keys())\n",
        "    for link in links[:max_articles - 1]:\n",
        "        linked_page = wiki.page(link)\n",
        "        if linked_page.exists() and len(linked_page.summary) > 100:\n",
        "            articles.append((linked_page.title, linked_page.summary + \"\\n\" + linked_page.text))\n",
        "        if len(articles) >= max_articles:\n",
        "            break\n",
        "    return articles\n",
        "\n",
        "# Scrape Wikipedia\n",
        "for category, topics in wiki_categories.items():\n",
        "    for topic in topics:\n",
        "        try:\n",
        "            print(f\"Extracting Wikipedia articles for: {topic} ({category})\")\n",
        "            articles = get_links(topic, max_per_topic // len(topics))\n",
        "            for title, content in articles:\n",
        "                wikipedia_docs.append({\n",
        "                    \"title\": title,\n",
        "                    \"source\": \"Wikipedia\",\n",
        "                    \"category\": category,\n",
        "                    \"content\": content\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Failed for {topic}: {e}\")\n",
        "        time.sleep(0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcy-HMbncUbI",
        "outputId": "d28d1014-3022-4935-c654-31fcf00a106f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting Wikipedia articles for: Artificial neural network (Machine learning)\n",
            "Extracting Wikipedia articles for: Supervised learning (Machine learning)\n",
            "Extracting Wikipedia articles for: Unsupervised learning (Machine learning)\n",
            "Extracting Wikipedia articles for: Climate change (Climate change)\n",
            "Extracting Wikipedia articles for: Carbon dioxide (Climate change)\n",
            "Extracting Wikipedia articles for: Climate change mitigation (Climate change)\n",
            "Extracting Wikipedia articles for: Greenhouse gas (Climate change)\n",
            "Extracting Wikipedia articles for: Sports (Sports)\n",
            "Extracting Wikipedia articles for: Football (Sports)\n",
            "Extracting Wikipedia articles for: Olympic Games (Sports)\n",
            "Extracting Wikipedia articles for: Basketball (Sports)\n",
            "Extracting Wikipedia articles for: Cricket (Sports)\n",
            "Extracting Wikipedia articles for: Political science (Politics)\n",
            "Extracting Wikipedia articles for: Elections (Politics)\n",
            "Extracting Wikipedia articles for: Political party (Politics)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RSS News Collection**\n",
        "\n",
        "**Define RSS feed sources (rss_feeds)**\n",
        "\n",
        "BBC feeds: Technology, Health, Science, World\n",
        "\n",
        "Guardian feeds: UK News, World\n",
        "\n",
        "These adds real-world, up-to-date news articles to the dataset.\n",
        "\n",
        "**Parse feeds**\n",
        "\n",
        "Use feedparser to download RSS feeds.\n",
        "\n",
        "For each entry: collect title + summary.\n",
        "\n",
        "Store in a dictionary:\n",
        "\n",
        "title, source = RSS, category (feed name), content.\n",
        "\n",
        "Hundreds of fresh news articles with summaries."
      ],
      "metadata": {
        "id": "x5D4drIPx6qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- RSS Setup -----------\n",
        "rss_feeds = {\n",
        "    \"BBC_Technology\": \"http://feeds.bbci.co.uk/news/technology/rss.xml\",\n",
        "    \"BBC_Health\": \"http://feeds.bbci.co.uk/news/health/rss.xml\",\n",
        "    \"BBC_Science\": \"http://feeds.bbci.co.uk/news/science_and_environment/rss.xml\",\n",
        "    \"BBC_World\": \"http://feeds.bbci.co.uk/news/world/rss.xml\",\n",
        "    \"Guardian_UK_News\": \"https://www.theguardian.com/uk-news/rss\",\n",
        "    \"Guardian_World\": \"https://www.theguardian.com/world/rss\"\n",
        "}\n",
        "\n",
        "rss_docs = []\n",
        "\n",
        "for category, url in rss_feeds.items():\n",
        "    print(f\"Parsing RSS feed: {category}\")\n",
        "    feed = feedparser.parse(url)\n",
        "    for entry in feed.entries:\n",
        "        content = entry.title + \". \" + (entry.summary if 'summary' in entry else \"\")\n",
        "        rss_docs.append({\n",
        "            \"title\": entry.title,\n",
        "            \"source\": \"RSS\",\n",
        "            \"category\": category,\n",
        "            \"content\": content\n",
        "        })\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYlwgu4OcUd_",
        "outputId": "54370385-8406-4959-cc2e-5483058451bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing RSS feed: BBC_Technology\n",
            "Parsing RSS feed: BBC_Health\n",
            "Parsing RSS feed: BBC_Science\n",
            "Parsing RSS feed: BBC_World\n",
            "Parsing RSS feed: Guardian_UK_News\n",
            "Parsing RSS feed: Guardian_World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine Wikipedia + RSS data into one pandas DataFrame.\n",
        "\n",
        "Remove duplicates (by title) to avoid redundancy.\n",
        "\n",
        "Reset index - keep dataset tidy.And\n",
        "\n",
        "Save dataset to combined_corpus.csv."
      ],
      "metadata": {
        "id": "bG_nzzhTyvFq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- Combine and Save -----------\n",
        "# Combine both datasets, remove duplicates by title\n",
        "all_docs = pd.DataFrame(wikipedia_docs + rss_docs)\n",
        "all_docs.drop_duplicates(subset=[\"title\"], inplace=True)\n",
        "all_docs.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(f\"Total unique documents: {len(all_docs)}\")\n",
        "all_docs.to_csv(\"combined_corpus.csv\", index=False)\n",
        "print(\"Saved as combined_corpus.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smpD8VaycUhH",
        "outputId": "9cba0e98-b8ee-492a-95e4-fe9a21b61583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique documents: 6563\n",
            "Saved as combined_corpus.csv\n"
          ]
        }
      ]
    }
  ]
}